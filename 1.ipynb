{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371582d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ARKK VBA data...\n",
      "Getting company names and industry info...\n",
      "Company names found for 170,323 records\n",
      "Industry info found for 161,049 records\n",
      "Processing price data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mfr/Library/CloudStorage/Dropbox/Robyn/Python_Projects/Finance_environment/finance_venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell RZ3 is marked as a date but the serial value 7916800000 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found price data for 434 stocks\n",
      "Filtering business days...\n",
      "Filtered: 170,323 -> 116,862 rows\n",
      "\n",
      "============================================================\n",
      "📊 FINAL DATAFRAME\n",
      "============================================================\n",
      "Shape: 116,862 rows × 11 columns\n",
      "Date range: 2014-10-31 to 2025-05-21\n",
      "\n",
      "Columns:\n",
      " 1. Name                 (116,862 non-null)\n",
      " 2. Weight               (116,862 non-null)\n",
      " 3. Position             (116,862 non-null)\n",
      " 4. Market Value         (116,862 non-null)\n",
      " 5. CUSIP                (104,900 non-null)\n",
      " 6. BBID                 (116,862 non-null)\n",
      " 7. ISIN                 (113,749 non-null)\n",
      " 8. Fund Flows           (116,862 non-null)\n",
      " 9. Company_Name         (116,862 non-null)\n",
      "10. Industry             (110,478 non-null)\n",
      "11. Stock_Price          (114,059 non-null)\n",
      "\n",
      "Sample data:\n",
      "                Company_Name                                  Industry  \\\n",
      "Date                                                                     \n",
      "2014-10-31  MercadoLibre Inc                          Broadline Retail   \n",
      "2014-10-31      Autodesk Inc                                  Software   \n",
      "2014-10-31    Proto Labs Inc                                 Machinery   \n",
      "2014-10-31     Wolfspeed Inc  Semiconductors & Semiconductor Equipment   \n",
      "2014-10-31       NVIDIA Corp  Semiconductors & Semiconductor Equipment   \n",
      "\n",
      "            Stock_Price  \n",
      "Date                     \n",
      "2014-10-31     136.1500  \n",
      "2014-10-31      57.5400  \n",
      "2014-10-31      65.3700  \n",
      "2014-10-31      31.4800  \n",
      "2014-10-31       0.4885  \n",
      "\n",
      "Stock_Price missing: 2,803 / 116,862 (2.4%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from workalendar.usa import UnitedStates\n",
    "\n",
    "cal = UnitedStates()\n",
    "\n",
    "# 1. Read VBA sheet as main data framework\n",
    "print(\"Reading ARKK VBA data...\")\n",
    "complete_data = pd.read_excel('ARKK521.xlsx', sheet_name='VBA')\n",
    "complete_data['Date'] = pd.to_datetime(complete_data['Date'])\n",
    "\n",
    "# 2. Extract company names and industry information for lookup\n",
    "print(\"Getting company names and industry info...\")\n",
    "stocks_info = pd.read_excel('arkstocksinfo.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Use Name (B column) to match and get H column (long_comp_name) content\n",
    "# B column = \"Bloomberg Name\", H column = \"long_comp_name\"\n",
    "name_lookup = stocks_info.drop_duplicates('Bloomberg Name').set_index('Bloomberg Name')['long_comp_name'].to_dict()\n",
    "industry_lookup = stocks_info.drop_duplicates('Bloomberg Name').set_index('Bloomberg Name')['gics_industry_name'].to_dict()\n",
    "\n",
    "# Add columns by mapping Name to company info\n",
    "complete_data['Company_Name'] = complete_data['Name'].map(name_lookup)\n",
    "complete_data['Industry'] = complete_data['Name'].map(industry_lookup)\n",
    "\n",
    "print(f\"Company names found for {complete_data['Company_Name'].notna().sum():,} records\")\n",
    "print(f\"Industry info found for {complete_data['Industry'].notna().sum():,} records\")\n",
    "\n",
    "# 3. Process price data\n",
    "print(\"Processing price data...\")\n",
    "df_raw = pd.read_excel('ALLARK521StocksNoDiv.xlsx', sheet_name='Sheet1', header=None)\n",
    "\n",
    "# Extract Bloomberg Name and prices\n",
    "price_lookup = {}\n",
    "for i in range(2, df_raw.shape[1], 2):\n",
    "    # Check if both date and price columns exist\n",
    "    if i < df_raw.shape[1] and (i + 1) < df_raw.shape[1]:\n",
    "        bloomberg_name = df_raw.iloc[1, i]  # Row 1: Bloomberg Name\n",
    "        if pd.notna(bloomberg_name):\n",
    "            stock_prices = {}\n",
    "            date_col = i\n",
    "            price_col = i + 1\n",
    "            \n",
    "            for row in range(2, df_raw.shape[0]):\n",
    "                # Add safety check for row access\n",
    "                if row < df_raw.shape[0]:\n",
    "                    date_val = df_raw.iloc[row, date_col]\n",
    "                    price_val = df_raw.iloc[row, price_col]\n",
    "                    \n",
    "                    if pd.notna(date_val) and pd.notna(price_val):\n",
    "                        try:\n",
    "                            date_obj = pd.to_datetime(date_val)\n",
    "                            stock_prices[date_obj] = float(price_val)\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            if stock_prices:\n",
    "                price_lookup[bloomberg_name] = stock_prices\n",
    "\n",
    "print(f\"Found price data for {len(price_lookup)} stocks\")\n",
    "\n",
    "# 4. Add stock price column\n",
    "complete_data['Stock_Price'] = complete_data.apply(\n",
    "    lambda row: price_lookup.get(row['Name'], {}).get(row['Date'], np.nan), axis=1\n",
    ")\n",
    "\n",
    "# 5. Filter business days only\n",
    "print(\"Filtering business days...\")\n",
    "original_count = len(complete_data)\n",
    "complete_data = complete_data[complete_data['Date'].apply(lambda x: cal.is_working_day(x.date()))]\n",
    "print(f\"Filtered: {original_count:,} -> {len(complete_data):,} rows\")\n",
    "\n",
    "# 6. Set date as index\n",
    "complete_data.set_index('Date', inplace=True)\n",
    "complete_data.sort_index(inplace=True)\n",
    "\n",
    "# 7. Display final results\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"📊 FINAL DATAFRAME\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {complete_data.shape[0]:,} rows × {complete_data.shape[1]} columns\")\n",
    "print(f\"Date range: {complete_data.index.min().strftime('%Y-%m-%d')} to {complete_data.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nColumns:\")\n",
    "for i, col in enumerate(complete_data.columns, 1):\n",
    "    non_null = complete_data[col].notna().sum()\n",
    "    print(f\"{i:2d}. {col:<20} ({non_null:,} non-null)\")\n",
    "\n",
    "print(f\"\\nSample data:\")\n",
    "print(complete_data[['Company_Name', 'Industry', 'Stock_Price']].head())\n",
    "\n",
    "# Statistics\n",
    "price_missing = complete_data['Stock_Price'].isnull().sum()\n",
    "print(f\"\\nStock_Price missing: {price_missing:,} / {len(complete_data):,} ({price_missing/len(complete_data)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b28201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 STEP 1: ANALYZING PRICE COVERAGE BY DATE\n",
      "============================================================\n",
      "Overall price coverage: 97.5%\n",
      "\n",
      "Worst 10 dates (price coverage):\n",
      "   2015-04-03: 1/57 (1.8%)\n",
      "   2021-04-02: 1/57 (1.8%)\n",
      "   2018-03-30: 1/53 (1.9%)\n",
      "   2017-04-14: 1/50 (2.0%)\n",
      "   2016-03-25: 1/43 (2.3%)\n",
      "   2018-12-05: 1/39 (2.6%)\n",
      "   2019-04-19: 1/39 (2.6%)\n",
      "   2024-03-29: 1/39 (2.6%)\n",
      "   2022-04-15: 1/37 (2.7%)\n",
      "   2025-04-18: 1/37 (2.7%)\n",
      "\n",
      "Removing 16 dates with <50.0% price coverage...\n",
      "✅ Removed 656 records from 16 dates\n",
      "⚠️  1 dates with weight sum ≠ 1\n",
      "\n",
      "DATA QUALITY CHECKS\n",
      "============================================================\n",
      "1. Days with >5 stocks having same price: 0/2632 (0.0%)\n",
      "\n",
      "2. Days with >5 stocks having same position: 3/2632 (0.1%)\n",
      "   Problem dates: 2015-02-06, 2020-03-19, 2021-03-25\n",
      "\n",
      "3. Days with >90% stocks unchanged positions: 692/2632 (26.3%)\n",
      "   Problem dates: 2014-11-03, 2014-11-04, 2014-11-05, 2014-11-06, 2014-11-10 (+687 more)\n",
      "\n",
      "4. Days with missing prices (excluding 100% missing tickers): 45/2632 (1.7%)\n",
      "   Worst dates: 2015-01-02, 2015-01-16, 2015-01-23, 2015-01-30, 2015-02-06 (+40 more)\n",
      "\n",
      "5. Tickers with missing prices: 12/220 (5.5%)\n",
      "   Worst tickers:\n",
      "     ARMH US Equity: 100.0% missing\n",
      "     FEDXX1Y US Equity: 100.0% missing\n",
      "     MRVXX US Equity: 100.0% missing\n",
      "     ONVO US Equity: 100.0% missing\n",
      "     ESLT IT Equity: 23.1% missing\n",
      "     EVGN IT Equity: 23.1% missing\n",
      "     SHOP CN Equity: 1.8% missing\n",
      "     MVRXX US Equity: 0.9% missing\n",
      "     JUNO US Equity: 0.5% missing\n",
      "     GBTC US Equity: 0.1% missing\n",
      "     CRSP US Equity: 0.1% missing\n",
      "     NVTAQ US Equity: 0.0% missing\n"
     ]
    }
   ],
   "source": [
    "# Complete ARK Data Processing Pipeline\n",
    "# 1. Remove low quality dates → 2. Clean data & calculate weights\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: ANALYZE AND REMOVE DATES WITH POOR PRICE COVERAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"📊 STEP 1: ANALYZING PRICE COVERAGE BY DATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate price coverage by date\n",
    "daily_stats = complete_data.groupby(complete_data.index).agg({\n",
    "    'Stock_Price': lambda x: x.notna().sum(),\n",
    "    'Name': 'count'\n",
    "})\n",
    "daily_stats.columns = ['Tickers_With_Price', 'Total_Tickers']\n",
    "daily_stats['Price_Coverage_Pct'] = (daily_stats['Tickers_With_Price'] / daily_stats['Total_Tickers'] * 100).round(1)\n",
    "\n",
    "print(f\"Overall price coverage: {daily_stats['Price_Coverage_Pct'].mean():.1f}%\")\n",
    "\n",
    "# Show worst dates\n",
    "worst_dates = daily_stats.nsmallest(10, 'Price_Coverage_Pct')\n",
    "print(f\"\\nWorst 10 dates (price coverage):\")\n",
    "for date, row in worst_dates.iterrows():\n",
    "    print(f\"   {date.strftime('%Y-%m-%d')}: {row['Tickers_With_Price']:.0f}/{row['Total_Tickers']:.0f} ({row['Price_Coverage_Pct']:.1f}%)\")\n",
    "\n",
    "# Remove dates with <50% coverage\n",
    "threshold = 50.0\n",
    "dates_to_remove = daily_stats[daily_stats['Price_Coverage_Pct'] < threshold].index\n",
    "\n",
    "if len(dates_to_remove) > 0:\n",
    "    print(f\"\\nRemoving {len(dates_to_remove)} dates with <{threshold}% price coverage...\")\n",
    "    original_count = len(complete_data)\n",
    "    complete_data = complete_data[~complete_data.index.isin(dates_to_remove)]\n",
    "    removed_count = original_count - len(complete_data)\n",
    "    print(f\"✅ Removed {removed_count:,} records from {len(dates_to_remove):,} dates\")\n",
    "else:\n",
    "    print(f\"✅ No dates with <{threshold}% coverage found\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: DATA CLEANING AND WEIGHT CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# Remove USD entries\n",
    "original_count = len(complete_data)\n",
    "complete_data = complete_data[complete_data['Name'] != 'USD Curncy']\n",
    "usd_removed = original_count - len(complete_data)\n",
    "\n",
    "# Recalculate Market Value\n",
    "complete_data['Market Value'] = np.nan\n",
    "mask = (complete_data['Position'].notna()) & (complete_data['Stock_Price'].notna())\n",
    "complete_data.loc[mask, 'Market Value'] = complete_data.loc[mask, 'Position'] * complete_data.loc[mask, 'Stock_Price']\n",
    "calculated_count = complete_data['Market Value'].notna().sum()\n",
    "\n",
    "# Calculate ETF Market Value and Weights\n",
    "complete_data['Weight'] = np.nan\n",
    "\n",
    "# Daily ETF market values\n",
    "daily_total_mv = complete_data.groupby(complete_data.index)['Market Value'].sum()\n",
    "complete_data['ETF Market Value'] = complete_data.index.map(daily_total_mv)\n",
    "\n",
    "# Calculate weights\n",
    "weight_mask = ((complete_data['Market Value'].notna()) & \n",
    "              (complete_data['ETF Market Value'].notna()) & \n",
    "              (complete_data['ETF Market Value'] != 0))\n",
    "complete_data.loc[weight_mask, 'Weight'] = (\n",
    "    complete_data.loc[weight_mask, 'Market Value'] / \n",
    "    complete_data.loc[weight_mask, 'ETF Market Value']\n",
    ")\n",
    "\n",
    "weight_calculated = complete_data['Weight'].notna().sum()\n",
    "\n",
    "# Validate weights\n",
    "weight_sums = complete_data.groupby(complete_data.index)['Weight'].sum()\n",
    "tolerance = 0.001\n",
    "problematic_dates = weight_sums[abs(weight_sums - 1.0) > tolerance]\n",
    "\n",
    "if len(problematic_dates) > 0:\n",
    "    print(f\"⚠️  {len(problematic_dates)} dates with weight sum ≠ 1\")\n",
    "else:\n",
    "    print(\"✅ All dates have weight sum ≈ 1.0\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: FINAL EXPORT TO EXCEL\n",
    "# ============================================================================\n",
    "\n",
    "# Prepare for export\n",
    "export_data = complete_data.reset_index()\n",
    "export_data['Date'] = export_data['Date'].dt.date\n",
    "\n",
    "# Remove unwanted columns\n",
    "columns_to_remove = ['CUSIP', 'ISIN', 'BBID']\n",
    "for col in columns_to_remove:\n",
    "    if col in export_data.columns:\n",
    "        export_data = export_data.drop(columns=[col])\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['Date', 'Name', 'Company_Name', 'Industry', 'Position', 'Stock_Price', \n",
    "                'Market Value', 'ETF Market Value', 'Weight', 'Fund']\n",
    "\n",
    "existing_cols = [col for col in column_order if col in export_data.columns]\n",
    "other_cols = [col for col in export_data.columns if col not in column_order]\n",
    "final_column_order = existing_cols + other_cols\n",
    "export_data = export_data[final_column_order]\n",
    "\n",
    "# Export to Excel\n",
    "filename = 'ark_data_complete_processed.xlsx'\n",
    "with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "    export_data.to_excel(writer, sheet_name='ARK_Data_Final', index=False)\n",
    "    \n",
    "    # Auto-adjust column widths\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['ARK_Data_Final']\n",
    "    \n",
    "    for column in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(str(cell.value))\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = min(max_length + 2, 50)\n",
    "        worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: DATA QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nDATA QUALITY CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Days with identical stock prices for multiple stocks (>5 stocks)\n",
    "daily_price_duplicates = complete_data.groupby(complete_data.index).apply(\n",
    "    lambda x: (x['Stock_Price'].dropna().value_counts() > 5).any() if len(x['Stock_Price'].dropna()) > 5 else False\n",
    ")\n",
    "days_with_duplicate_prices = daily_price_duplicates.sum()\n",
    "total_days = len(daily_price_duplicates)\n",
    "print(f\"1. Days with >5 stocks having same price: {days_with_duplicate_prices}/{total_days} ({days_with_duplicate_prices/total_days*100:.1f}%)\")\n",
    "\n",
    "if days_with_duplicate_prices > 0:\n",
    "    duplicate_price_dates = daily_price_duplicates[daily_price_duplicates].index\n",
    "    print(f\"   Problem dates: {', '.join(duplicate_price_dates[:5].strftime('%Y-%m-%d'))}\" + \n",
    "          (f\" (+{len(duplicate_price_dates)-5} more)\" if len(duplicate_price_dates) > 5 else \"\"))\n",
    "\n",
    "# Check 2: Days with identical positions for multiple stocks (>5 stocks)\n",
    "daily_position_duplicates = complete_data.groupby(complete_data.index).apply(\n",
    "    lambda x: (x['Position'].dropna().value_counts() > 5).any() if len(x['Position'].dropna()) > 5 else False\n",
    ")\n",
    "days_with_duplicate_positions = daily_position_duplicates.sum()\n",
    "print(f\"\\n2. Days with >5 stocks having same position: {days_with_duplicate_positions}/{total_days} ({days_with_duplicate_positions/total_days*100:.1f}%)\")\n",
    "\n",
    "if days_with_duplicate_positions > 0:\n",
    "    duplicate_position_dates = daily_position_duplicates[daily_position_duplicates].index\n",
    "    print(f\"   Problem dates: {', '.join(duplicate_position_dates[:5].strftime('%Y-%m-%d'))}\" + \n",
    "          (f\" (+{len(duplicate_position_dates)-5} more)\" if len(duplicate_position_dates) > 5 else \"\"))\n",
    "\n",
    "# Check 3: Days with >90% stocks having no position changes from previous day\n",
    "def check_position_changes():\n",
    "    # Calculate position changes for each stock\n",
    "    complete_data_sorted = complete_data.sort_index()\n",
    "    complete_data_sorted['Position_Prev'] = complete_data_sorted.groupby('Name')['Position'].shift(1)\n",
    "    complete_data_sorted['Position_Changed'] = (\n",
    "        complete_data_sorted['Position'] != complete_data_sorted['Position_Prev']\n",
    "    ) | complete_data_sorted['Position_Prev'].isna()\n",
    "    \n",
    "    # Calculate daily percentage of stocks with no position changes\n",
    "    daily_no_change_pct = complete_data_sorted.groupby(complete_data_sorted.index).apply(\n",
    "        lambda x: (~x['Position_Changed']).sum() / len(x) * 100 if len(x) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    return daily_no_change_pct > 90\n",
    "\n",
    "daily_position_no_change = check_position_changes()\n",
    "days_no_position_change = daily_position_no_change.sum()\n",
    "print(f\"\\n3. Days with >90% stocks unchanged positions: {days_no_position_change}/{total_days} ({days_no_position_change/total_days*100:.1f}%)\")\n",
    "\n",
    "if days_no_position_change > 0:\n",
    "    no_change_dates = daily_position_no_change[daily_position_no_change].index\n",
    "    print(f\"   Problem dates: {', '.join(no_change_dates[:5].strftime('%Y-%m-%d'))}\" + \n",
    "          (f\" (+{len(no_change_dates)-5} more)\" if len(no_change_dates) > 5 else \"\"))\n",
    "\n",
    "# Check 4: Days missing stock price information (excluding 100% missing tickers)\n",
    "# First identify tickers with 100% missing prices\n",
    "ticker_price_coverage = complete_data.groupby('Name')['Stock_Price'].apply(\n",
    "    lambda x: x.notna().sum() / len(x) * 100\n",
    ")\n",
    "tickers_with_some_prices = ticker_price_coverage[ticker_price_coverage > 0].index\n",
    "\n",
    "# Only count missing prices for tickers that have some price data\n",
    "filtered_data = complete_data[complete_data['Name'].isin(tickers_with_some_prices)]\n",
    "daily_missing_prices = filtered_data.groupby(filtered_data.index)['Stock_Price'].apply(\n",
    "    lambda x: x.isna().sum()\n",
    ")\n",
    "days_with_missing_prices = (daily_missing_prices > 0).sum()\n",
    "print(f\"\\n4. Days with missing prices (excluding 100% missing tickers): {days_with_missing_prices}/{total_days} ({days_with_missing_prices/total_days*100:.1f}%)\")\n",
    "\n",
    "if days_with_missing_prices > 0:\n",
    "    worst_missing_days = daily_missing_prices[daily_missing_prices > 0].sort_values(ascending=False)\n",
    "    print(f\"   Worst dates: {', '.join(worst_missing_days.head(5).index.strftime('%Y-%m-%d'))}\" + \n",
    "          (f\" (+{len(worst_missing_days)-5} more)\" if len(worst_missing_days) > 5 else \"\"))\n",
    "\n",
    "# Check 5: Tickers missing stock price information\n",
    "ticker_missing_prices = complete_data.groupby('Name')['Stock_Price'].apply(\n",
    "    lambda x: x.isna().sum() / len(x) * 100\n",
    ")\n",
    "tickers_with_missing = ticker_missing_prices[ticker_missing_prices > 0].sort_values(ascending=False)\n",
    "print(f\"\\n5. Tickers with missing prices: {len(tickers_with_missing)}/{len(ticker_missing_prices)} ({len(tickers_with_missing)/len(ticker_missing_prices)*100:.1f}%)\")\n",
    "\n",
    "if len(tickers_with_missing) > 0:\n",
    "    print(f\"   Worst tickers:\")\n",
    "    for ticker, missing_pct in tickers_with_missing.head(20).items():\n",
    "        print(f\"     {ticker}: {missing_pct:.1f}% missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b177f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 113650 entries, 2014-10-31 to 2025-05-21\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Name              113650 non-null  object \n",
      " 1   Weight            111427 non-null  float64\n",
      " 2   Position          113650 non-null  float64\n",
      " 3   Market Value      111483 non-null  float64\n",
      " 4   CUSIP             104313 non-null  object \n",
      " 5   BBID              113650 non-null  object \n",
      " 6   ISIN              113115 non-null  object \n",
      " 7   Fund Flows        113650 non-null  float64\n",
      " 8   Company_Name      113650 non-null  object \n",
      " 9   Industry          109860 non-null  object \n",
      " 10  Stock_Price       111483 non-null  float64\n",
      " 11  ETF Market Value  113650 non-null  float64\n",
      "dtypes: float64(6), object(6)\n",
      "memory usage: 11.3+ MB\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Position</th>\n",
       "      <th>Market Value</th>\n",
       "      <th>CUSIP</th>\n",
       "      <th>BBID</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Fund Flows</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Stock_Price</th>\n",
       "      <th>ETF Market Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>MELI US Equity</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>424.0</td>\n",
       "      <td>57727.60</td>\n",
       "      <td>58733R102</td>\n",
       "      <td>BBG000GQPB11</td>\n",
       "      <td>US58733R1023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MercadoLibre Inc</td>\n",
       "      <td>Broadline Retail</td>\n",
       "      <td>136.1500</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>ADSK US Equity</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>80556.00</td>\n",
       "      <td>52769106</td>\n",
       "      <td>BBG000BM7HL0</td>\n",
       "      <td>US0527691069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Autodesk Inc</td>\n",
       "      <td>Software</td>\n",
       "      <td>57.5400</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>PRLB US Equity</td>\n",
       "      <td>0.026278</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>103546.08</td>\n",
       "      <td>743713109</td>\n",
       "      <td>BBG000BT13B3</td>\n",
       "      <td>US7437131094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Proto Labs Inc</td>\n",
       "      <td>Machinery</td>\n",
       "      <td>65.3700</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>WOLF US Equity</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>38279.68</td>\n",
       "      <td>977852102</td>\n",
       "      <td>BBG000BG14P4</td>\n",
       "      <td>US9778521024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wolfspeed Inc</td>\n",
       "      <td>Semiconductors &amp; Semiconductor Equipment</td>\n",
       "      <td>31.4800</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>NVDA US Equity</td>\n",
       "      <td>0.030784</td>\n",
       "      <td>248320.0</td>\n",
       "      <td>121304.32</td>\n",
       "      <td>67066G104</td>\n",
       "      <td>BBG000BBJQV0</td>\n",
       "      <td>US67066G1040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NVIDIA Corp</td>\n",
       "      <td>Semiconductors &amp; Semiconductor Equipment</td>\n",
       "      <td>0.4885</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name    Weight  Position  Market Value      CUSIP  \\\n",
       "Date                                                                      \n",
       "2014-10-31  MELI US Equity  0.014650     424.0      57727.60  58733R102   \n",
       "2014-10-31  ADSK US Equity  0.020443    1400.0      80556.00   52769106   \n",
       "2014-10-31  PRLB US Equity  0.026278    1584.0     103546.08  743713109   \n",
       "2014-10-31  WOLF US Equity  0.009714    1216.0      38279.68  977852102   \n",
       "2014-10-31  NVDA US Equity  0.030784  248320.0     121304.32  67066G104   \n",
       "\n",
       "                    BBID          ISIN  Fund Flows      Company_Name  \\\n",
       "Date                                                                   \n",
       "2014-10-31  BBG000GQPB11  US58733R1023         0.0  MercadoLibre Inc   \n",
       "2014-10-31  BBG000BM7HL0  US0527691069         0.0      Autodesk Inc   \n",
       "2014-10-31  BBG000BT13B3  US7437131094         0.0    Proto Labs Inc   \n",
       "2014-10-31  BBG000BG14P4  US9778521024         0.0     Wolfspeed Inc   \n",
       "2014-10-31  BBG000BBJQV0  US67066G1040         0.0       NVIDIA Corp   \n",
       "\n",
       "                                            Industry  Stock_Price  \\\n",
       "Date                                                                \n",
       "2014-10-31                          Broadline Retail     136.1500   \n",
       "2014-10-31                                  Software      57.5400   \n",
       "2014-10-31                                 Machinery      65.3700   \n",
       "2014-10-31  Semiconductors & Semiconductor Equipment      31.4800   \n",
       "2014-10-31  Semiconductors & Semiconductor Equipment       0.4885   \n",
       "\n",
       "            ETF Market Value  \n",
       "Date                          \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"DataFrame Info:\")\n",
    "complete_data.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b670d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ARK Small Positions Analysis exported to 'ARK_Small_Positions_Analysis.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# ARK Portfolio Analysis - Small Positions (<1%) Performance Study\n",
    "# Export all results to Excel without console output\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Filter positions with weight < 1%\n",
    "small_positions = complete_data[complete_data['Weight'] < 0.01].copy()\n",
    "large_positions = complete_data[complete_data['Weight'] >= 0.01].copy()\n",
    "\n",
    "# Step 2: Calculate daily returns\n",
    "def calculate_returns(df):\n",
    "    df = df.copy().sort_index()\n",
    "    df['Price_Return'] = df.groupby('Name')['Stock_Price'].pct_change()\n",
    "    return df\n",
    "\n",
    "complete_data_with_returns = calculate_returns(complete_data)\n",
    "\n",
    "# Filter out invalid returns\n",
    "valid_returns_mask = (\n",
    "    (complete_data_with_returns['Price_Return'].notna()) & \n",
    "    (abs(complete_data_with_returns['Price_Return']) < 2.0)\n",
    ")\n",
    "\n",
    "complete_data_clean = complete_data_with_returns[valid_returns_mask]\n",
    "small_positions = complete_data_clean[complete_data_clean['Weight'] < 0.01].copy()\n",
    "large_positions = complete_data_clean[complete_data_clean['Weight'] >= 0.01].copy()\n",
    "\n",
    "small_returns = small_positions['Price_Return']\n",
    "large_returns = large_positions['Price_Return']\n",
    "\n",
    "# Step 3: Calculate market value changes\n",
    "small_positions['Daily_MV_Change'] = small_positions['Market Value'] * small_positions['Price_Return']\n",
    "large_positions['Daily_MV_Change'] = large_positions['Market Value'] * large_positions['Price_Return']\n",
    "\n",
    "small_mv_clean = small_positions['Daily_MV_Change'].dropna()\n",
    "large_mv_clean = large_positions['Daily_MV_Change'].dropna()\n",
    "small_mv_clean = small_mv_clean[abs(small_mv_clean) < small_mv_clean.quantile(0.999)]\n",
    "large_mv_clean = large_mv_clean[abs(large_mv_clean) < large_mv_clean.quantile(0.999)]\n",
    "\n",
    "# Step 4: Calculate Slugging Ratios\n",
    "small_winning_returns = small_returns[small_returns > 0]\n",
    "small_losing_returns = small_returns[small_returns < 0]\n",
    "small_avg_winning_return = small_winning_returns.mean()\n",
    "small_avg_losing_return = abs(small_losing_returns.mean())\n",
    "small_slugging_ratio_returns = small_avg_winning_return / small_avg_losing_return if small_avg_losing_return != 0 else np.inf\n",
    "\n",
    "large_winning_returns = large_returns[large_returns > 0]\n",
    "large_losing_returns = large_returns[large_returns < 0]\n",
    "large_avg_winning_return = large_winning_returns.mean()\n",
    "large_avg_losing_return = abs(large_losing_returns.mean())\n",
    "large_slugging_ratio_returns = large_avg_winning_return / large_avg_losing_return if large_avg_losing_return != 0 else np.inf\n",
    "\n",
    "# Win/Loss analysis\n",
    "small_positive_days = (small_mv_clean > 0).sum()\n",
    "small_negative_days = (small_mv_clean < 0).sum()\n",
    "small_win_rate = small_positive_days / len(small_mv_clean)\n",
    "\n",
    "large_positive_days = (large_mv_clean > 0).sum()\n",
    "large_negative_days = (large_mv_clean < 0).sum()\n",
    "large_win_rate = large_positive_days / len(large_mv_clean)\n",
    "\n",
    "# Extreme values\n",
    "small_best_day = small_mv_clean.max()\n",
    "small_worst_day = small_mv_clean.min()\n",
    "large_best_day = large_mv_clean.max()\n",
    "large_worst_day = large_mv_clean.min()\n",
    "\n",
    "small_total_change = small_mv_clean.sum()\n",
    "large_total_change = large_mv_clean.sum()\n",
    "\n",
    "small_daily_avg = small_mv_clean.mean()\n",
    "large_daily_avg = large_mv_clean.mean()\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT TO EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "excel_filename = 'ARK_Small_Positions_Analysis.xlsx'\n",
    "with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "    \n",
    "    # Sheet 1: Summary Statistics\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Records',\n",
    "            'Records with Valid Returns',\n",
    "            'Mean Daily Return (%)',\n",
    "            'Median Daily Return (%)',\n",
    "            'Standard Deviation (%)',\n",
    "            'Min Daily Return (%)',\n",
    "            'Max Daily Return (%)',\n",
    "            '1st Percentile (%)',\n",
    "            '5th Percentile (%)',\n",
    "            '25th Percentile (%)',\n",
    "            '75th Percentile (%)',\n",
    "            '95th Percentile (%)',\n",
    "            '99th Percentile (%)',\n",
    "            'Win Rate (%)',\n",
    "            'Average Winning Return (%)',\n",
    "            'Average Losing Return (%)',\n",
    "            'Slugging Ratio',\n",
    "            'Total MV Change ($)',\n",
    "            'Average Daily MV Change ($)',\n",
    "            'Best Day MV Change ($)',\n",
    "            'Worst Day MV Change ($)',\n",
    "            'Risk-Adjusted Return'\n",
    "        ],\n",
    "        'Small Positions (<1%)': [\n",
    "            len(complete_data[complete_data['Weight'] < 0.01]),\n",
    "            len(small_returns),\n",
    "            f\"{small_returns.mean()*100:.4f}\",\n",
    "            f\"{small_returns.median()*100:.4f}\",\n",
    "            f\"{small_returns.std()*100:.4f}\",\n",
    "            f\"{small_returns.min()*100:.4f}\",\n",
    "            f\"{small_returns.max()*100:.4f}\",\n",
    "            f\"{np.percentile(small_returns, 1)*100:.4f}\",\n",
    "            f\"{np.percentile(small_returns, 5)*100:.4f}\",\n",
    "            f\"{np.percentile(small_returns, 25)*100:.4f}\",\n",
    "            f\"{np.percentile(small_returns, 75)*100:.4f}\",\n",
    "            f\"{np.percentile(small_returns, 95)*100:.4f}\",\n",
    "            f\"{np.percentile(small_returns, 99)*100:.4f}\",\n",
    "            f\"{small_win_rate*100:.2f}\",\n",
    "            f\"{small_avg_winning_return*100:.4f}\",\n",
    "            f\"{small_avg_losing_return*100:.4f}\",\n",
    "            f\"{small_slugging_ratio_returns:.4f}\",\n",
    "            f\"{small_total_change:,.2f}\",\n",
    "            f\"{small_daily_avg:,.2f}\",\n",
    "            f\"{small_best_day:,.2f}\",\n",
    "            f\"{small_worst_day:,.2f}\",\n",
    "            f\"{(small_returns.mean()/small_returns.std()):.4f}\"\n",
    "        ],\n",
    "        'Large Positions (≥1%)': [\n",
    "            len(complete_data[complete_data['Weight'] >= 0.01]),\n",
    "            len(large_returns),\n",
    "            f\"{large_returns.mean()*100:.4f}\",\n",
    "            f\"{large_returns.median()*100:.4f}\",\n",
    "            f\"{large_returns.std()*100:.4f}\",\n",
    "            f\"{large_returns.min()*100:.4f}\",\n",
    "            f\"{large_returns.max()*100:.4f}\",\n",
    "            f\"{np.percentile(large_returns, 1)*100:.4f}\",\n",
    "            f\"{np.percentile(large_returns, 5)*100:.4f}\",\n",
    "            f\"{np.percentile(large_returns, 25)*100:.4f}\",\n",
    "            f\"{np.percentile(large_returns, 75)*100:.4f}\",\n",
    "            f\"{np.percentile(large_returns, 95)*100:.4f}\",\n",
    "            f\"{np.percentile(large_returns, 99)*100:.4f}\",\n",
    "            f\"{large_win_rate*100:.2f}\",\n",
    "            f\"{large_avg_winning_return*100:.4f}\",\n",
    "            f\"{large_avg_losing_return*100:.4f}\",\n",
    "            f\"{large_slugging_ratio_returns:.4f}\",\n",
    "            f\"{large_total_change:,.2f}\",\n",
    "            f\"{large_daily_avg:,.2f}\",\n",
    "            f\"{large_best_day:,.2f}\",\n",
    "            f\"{large_worst_day:,.2f}\",\n",
    "            f\"{(large_returns.mean()/large_returns.std()):.4f}\"\n",
    "        ],\n",
    "        'Difference (Small - Large)': [\n",
    "            len(complete_data[complete_data['Weight'] < 0.01]) - len(complete_data[complete_data['Weight'] >= 0.01]),\n",
    "            len(small_returns) - len(large_returns),\n",
    "            f\"{(small_returns.mean() - large_returns.mean())*100:.4f}\",\n",
    "            f\"{(small_returns.median() - large_returns.median())*100:.4f}\",\n",
    "            f\"{(small_returns.std() - large_returns.std())*100:.4f}\",\n",
    "            f\"{(small_returns.min() - large_returns.min())*100:.4f}\",\n",
    "            f\"{(small_returns.max() - large_returns.max())*100:.4f}\",\n",
    "            f\"{(np.percentile(small_returns, 1) - np.percentile(large_returns, 1))*100:.4f}\",\n",
    "            f\"{(np.percentile(small_returns, 5) - np.percentile(large_returns, 5))*100:.4f}\",\n",
    "            f\"{(np.percentile(small_returns, 25) - np.percentile(large_returns, 25))*100:.4f}\",\n",
    "            f\"{(np.percentile(small_returns, 75) - np.percentile(large_returns, 75))*100:.4f}\",\n",
    "            f\"{(np.percentile(small_returns, 95) - np.percentile(large_returns, 95))*100:.4f}\",\n",
    "            f\"{(np.percentile(small_returns, 99) - np.percentile(large_returns, 99))*100:.4f}\",\n",
    "            f\"{(small_win_rate - large_win_rate)*100:.2f}\",\n",
    "            f\"{(small_avg_winning_return - large_avg_winning_return)*100:.4f}\",\n",
    "            f\"{(small_avg_losing_return - large_avg_losing_return)*100:.4f}\",\n",
    "            f\"{small_slugging_ratio_returns - large_slugging_ratio_returns:.4f}\",\n",
    "            f\"{small_total_change - large_total_change:,.2f}\",\n",
    "            f\"{small_daily_avg - large_daily_avg:,.2f}\",\n",
    "            f\"{small_best_day - large_best_day:,.2f}\",\n",
    "            f\"{small_worst_day - large_worst_day:,.2f}\",\n",
    "            f\"{(small_returns.mean()/small_returns.std()) - (large_returns.mean()/large_returns.std()):.4f}\"\n",
    "        ],\n",
    "        'Winner': [\n",
    "            'Small' if len(complete_data[complete_data['Weight'] < 0.01]) > len(complete_data[complete_data['Weight'] >= 0.01]) else 'Large',\n",
    "            'Small' if len(small_returns) > len(large_returns) else 'Large',\n",
    "            'Small' if small_returns.mean() > large_returns.mean() else 'Large',\n",
    "            'Small' if small_returns.median() > large_returns.median() else 'Large',\n",
    "            'Small' if small_returns.std() < large_returns.std() else 'Large',\n",
    "            'Small' if small_returns.min() > large_returns.min() else 'Large',\n",
    "            'Small' if small_returns.max() > large_returns.max() else 'Large',\n",
    "            'Small' if np.percentile(small_returns, 1) > np.percentile(large_returns, 1) else 'Large',\n",
    "            'Small' if np.percentile(small_returns, 5) > np.percentile(large_returns, 5) else 'Large',\n",
    "            'Small' if np.percentile(small_returns, 25) > np.percentile(large_returns, 25) else 'Large',\n",
    "            'Small' if np.percentile(small_returns, 75) > np.percentile(large_returns, 75) else 'Large',\n",
    "            'Small' if np.percentile(small_returns, 95) > np.percentile(large_returns, 95) else 'Large',\n",
    "            'Small' if np.percentile(small_returns, 99) > np.percentile(large_returns, 99) else 'Large',\n",
    "            'Small' if small_win_rate > large_win_rate else 'Large',\n",
    "            'Small' if small_avg_winning_return > large_avg_winning_return else 'Large',\n",
    "            'Small' if small_avg_losing_return < large_avg_losing_return else 'Large',\n",
    "            'Small' if small_slugging_ratio_returns > large_slugging_ratio_returns else 'Large',\n",
    "            'Small' if small_total_change > large_total_change else 'Large',\n",
    "            'Small' if small_daily_avg > large_daily_avg else 'Large',\n",
    "            'Small' if small_best_day > large_best_day else 'Large',\n",
    "            'Small' if small_worst_day > large_worst_day else 'Large',\n",
    "            'Small' if (small_returns.mean()/small_returns.std()) > (large_returns.mean()/large_returns.std()) else 'Large'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary_Analysis', index=False)\n",
    "    \n",
    "    # Sheet 2: Key Insights\n",
    "    insights_data = {\n",
    "        'Performance Category': [\n",
    "            'Overall Daily Performance',\n",
    "            'Risk Management (Lower Volatility)', \n",
    "            'Consistency (Higher Win Rate)',\n",
    "            'Winner Quality (Slugging Ratio)',\n",
    "            'Extreme Day Performance',\n",
    "            'Market Value Generation',\n",
    "            'Risk-Adjusted Returns',\n",
    "            'Final Verdict'\n",
    "        ],\n",
    "        'Winner': [\n",
    "            'Small Positions' if small_returns.mean() > large_returns.mean() else 'Large Positions',\n",
    "            'Small Positions' if small_returns.std() < large_returns.std() else 'Large Positions',\n",
    "            'Small Positions' if small_win_rate > large_win_rate else 'Large Positions',\n",
    "            'Small Positions' if small_slugging_ratio_returns > large_slugging_ratio_returns else 'Large Positions',\n",
    "            'Small Positions' if small_returns.max() > large_returns.max() else 'Large Positions',\n",
    "            'Small Positions' if small_total_change > large_total_change else 'Large Positions',\n",
    "            'Small Positions' if (small_returns.mean()/small_returns.std()) > (large_returns.mean()/large_returns.std()) else 'Large Positions',\n",
    "            'Small Positions' if sum([\n",
    "                small_returns.mean() > large_returns.mean(),\n",
    "                small_returns.std() < large_returns.std(),\n",
    "                small_win_rate > large_win_rate,\n",
    "                small_slugging_ratio_returns > large_slugging_ratio_returns,\n",
    "                small_total_change > large_total_change,\n",
    "                (small_returns.mean()/small_returns.std()) > (large_returns.mean()/large_returns.std())\n",
    "            ]) >= 4 else 'Large Positions'\n",
    "        ],\n",
    "        'Key Insight': [\n",
    "            f\"Daily edge of {abs(small_returns.mean() - large_returns.mean())*100:.3f}% per day\",\n",
    "            f\"Daily volatility {'advantage' if small_returns.std() < large_returns.std() else 'disadvantage'} of {abs(small_returns.std() - large_returns.std())*100:.2f}%\",\n",
    "            f\"Win {abs(small_win_rate - large_win_rate)*100:.1f}% more trading days\",\n",
    "            f\"Winners generate {max(small_slugging_ratio_returns, large_slugging_ratio_returns):.2f}x vs losers daily\",\n",
    "            f\"Best day advantage: {abs(small_returns.max() - large_returns.max())*100:.2f}%\",\n",
    "            f\"Total dollar advantage: ${abs(small_total_change - large_total_change):,.0f}\",\n",
    "            f\"Better risk-return by {abs((small_returns.mean()/small_returns.std()) - (large_returns.mean()/large_returns.std())):.3f}\",\n",
    "            f\"Dominate {sum([small_returns.mean() > large_returns.mean(), small_returns.std() < large_returns.std(), small_win_rate > large_win_rate, small_slugging_ratio_returns > large_slugging_ratio_returns, small_total_change > large_total_change, (small_returns.mean()/small_returns.std()) > (large_returns.mean()/large_returns.std())])}/6 key metrics\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    insights_df = pd.DataFrame(insights_data)\n",
    "    insights_df.to_excel(writer, sheet_name='Key_Insights', index=False)\n",
    "\n",
    "# Auto-adjust column widths\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "wb = load_workbook(excel_filename)\n",
    "for sheet_name in wb.sheetnames:\n",
    "    ws = wb[sheet_name]\n",
    "    for column in ws.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            try:\n",
    "                max_length = max(max_length, len(str(cell.value)))\n",
    "            except:\n",
    "                pass\n",
    "        ws.column_dimensions[column_letter].width = min(max_length + 2, 50)\n",
    "\n",
    "wb.save(excel_filename)\n",
    "\n",
    "print(f\"✅ ARK Small Positions Analysis exported to '{excel_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288debdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to Small_Positions_Stock_Analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Export small positions stock analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filter stocks that were ever < 1% weight\n",
    "small_positions_data = complete_data_clean[complete_data_clean['Weight'] < 0.01].copy()\n",
    "\n",
    "# Calculate daily MV change\n",
    "small_positions_data['Daily_MV_Change'] = small_positions_data['Market Value'] * small_positions_data['Price_Return']\n",
    "\n",
    "# Get unique tickers that were ever small positions\n",
    "small_position_tickers = small_positions_data['Name'].unique()\n",
    "\n",
    "# Calculate metrics for each stock\n",
    "stock_results = []\n",
    "\n",
    "for ticker in small_position_tickers:\n",
    "    # Get data for this ticker when weight < 1%\n",
    "    ticker_data = small_positions_data[small_positions_data['Name'] == ticker].copy()\n",
    "    \n",
    "    # Get company name\n",
    "    company_name = ticker_data['Company_Name'].dropna().iloc[0] if len(ticker_data['Company_Name'].dropna()) > 0 else 'N/A'\n",
    "    \n",
    "    # Calculate metrics\n",
    "    returns = ticker_data['Price_Return'].dropna()\n",
    "    mv_changes = ticker_data['Daily_MV_Change'].dropna()\n",
    "    \n",
    "    if len(returns) == 0:\n",
    "        continue\n",
    "    \n",
    "    mean_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    total_mv_change = mv_changes.sum()\n",
    "    \n",
    "    # Calculate slugging ratio\n",
    "    winning_returns = returns[returns > 0]\n",
    "    losing_returns = returns[returns < 0]\n",
    "    \n",
    "    if len(winning_returns) > 0 and len(losing_returns) > 0:\n",
    "        avg_winning_return = winning_returns.mean()\n",
    "        avg_losing_return = abs(losing_returns.mean())\n",
    "        slugging_ratio = avg_winning_return / avg_losing_return if avg_losing_return != 0 else np.inf\n",
    "    else:\n",
    "        slugging_ratio = np.nan\n",
    "    \n",
    "    stock_results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company Name': company_name,\n",
    "        'Total MV Change ($)': total_mv_change,\n",
    "        'Slugging Ratio': slugging_ratio,\n",
    "        'Mean Daily Return (%)': mean_return * 100,\n",
    "        'Standard Deviation (%)': std_return * 100\n",
    "    })\n",
    "\n",
    "# Create dataframe\n",
    "results_df = pd.DataFrame(stock_results)\n",
    "\n",
    "# Export to excel\n",
    "excel_filename = 'Small_Positions_Stock_Analysis.xlsx'\n",
    "results_df.to_excel(excel_filename, index=False)\n",
    "\n",
    "print(f\"Exported to {excel_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
