{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371582d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ARKK VBA data...\n",
      "Getting company names and industry info...\n",
      "Company names found for 170,323 records\n",
      "Industry info found for 161,049 records\n",
      "Processing price data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mfr/Library/CloudStorage/Dropbox/Robyn/Python_Projects/Finance_environment/finance_venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell RZ3 is marked as a date but the serial value 7916800000 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found price data for 433 stocks\n",
      "Filtering business days...\n",
      "Filtered: 170,323 -> 116,862 rows\n",
      "\n",
      "============================================================\n",
      "📊 FINAL DATAFRAME\n",
      "============================================================\n",
      "Shape: 116,862 rows × 11 columns\n",
      "Date range: 2014-10-31 to 2025-05-21\n",
      "\n",
      "Columns:\n",
      " 1. Name                 (116,862 non-null)\n",
      " 2. Weight               (116,862 non-null)\n",
      " 3. Position             (116,862 non-null)\n",
      " 4. Market Value         (116,862 non-null)\n",
      " 5. CUSIP                (104,900 non-null)\n",
      " 6. BBID                 (116,862 non-null)\n",
      " 7. ISIN                 (113,749 non-null)\n",
      " 8. Fund Flows           (116,862 non-null)\n",
      " 9. Company_Name         (116,862 non-null)\n",
      "10. Industry             (110,478 non-null)\n",
      "11. Stock_Price          (115,270 non-null)\n",
      "\n",
      "Sample data:\n",
      "                Company_Name                                  Industry  \\\n",
      "Date                                                                     \n",
      "2014-10-31  MercadoLibre Inc                          Broadline Retail   \n",
      "2014-10-31      Autodesk Inc                                  Software   \n",
      "2014-10-31    Proto Labs Inc                                 Machinery   \n",
      "2014-10-31     Wolfspeed Inc  Semiconductors & Semiconductor Equipment   \n",
      "2014-10-31       NVIDIA Corp  Semiconductors & Semiconductor Equipment   \n",
      "\n",
      "            Stock_Price  \n",
      "Date                     \n",
      "2014-10-31     136.1500  \n",
      "2014-10-31      57.5400  \n",
      "2014-10-31      65.3700  \n",
      "2014-10-31      31.4800  \n",
      "2014-10-31       0.4885  \n",
      "\n",
      "Stock_Price missing: 1,592 / 116,862 (1.4%)\n",
      "\n",
      "✅ Saved to merged_ark_data_clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from workalendar.usa import UnitedStates\n",
    "\n",
    "cal = UnitedStates()\n",
    "\n",
    "# 1. Read VBA sheet as main data framework\n",
    "print(\"Reading ARKK VBA data...\")\n",
    "complete_data = pd.read_excel('ARKK521.xlsx', sheet_name='VBA')\n",
    "complete_data['Date'] = pd.to_datetime(complete_data['Date'])\n",
    "\n",
    "# 2. Extract company names and industry information for lookup\n",
    "print(\"Getting company names and industry info...\")\n",
    "stocks_info = pd.read_excel('arkstocksinfo.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Use Name (B column) to match and get H column (long_comp_name) content\n",
    "# B column = \"Bloomberg Name\", H column = \"long_comp_name\"\n",
    "name_lookup = stocks_info.drop_duplicates('Bloomberg Name').set_index('Bloomberg Name')['long_comp_name'].to_dict()\n",
    "industry_lookup = stocks_info.drop_duplicates('Bloomberg Name').set_index('Bloomberg Name')['gics_industry_name'].to_dict()\n",
    "\n",
    "# Add columns by mapping Name to company info\n",
    "complete_data['Company_Name'] = complete_data['Name'].map(name_lookup)\n",
    "complete_data['Industry'] = complete_data['Name'].map(industry_lookup)\n",
    "\n",
    "print(f\"Company names found for {complete_data['Company_Name'].notna().sum():,} records\")\n",
    "print(f\"Industry info found for {complete_data['Industry'].notna().sum():,} records\")\n",
    "\n",
    "# 3. Process price data\n",
    "print(\"Processing price data...\")\n",
    "df_raw = pd.read_excel('ALLARK521Stocks.xlsx', sheet_name='Sheet1', header=None)\n",
    "\n",
    "# Extract BBID and prices\n",
    "price_lookup = {}\n",
    "for i in range(2, df_raw.shape[1], 2):\n",
    "    # Check if both date and price columns exist\n",
    "    if i < df_raw.shape[1] and (i + 1) < df_raw.shape[1]:\n",
    "        bbid = df_raw.iloc[0, i]\n",
    "        if pd.notna(bbid) and str(bbid).startswith('BBG'):\n",
    "            stock_prices = {}\n",
    "            date_col = i\n",
    "            price_col = i + 1\n",
    "            \n",
    "            for row in range(2, df_raw.shape[0]):\n",
    "                # Add safety check for row access\n",
    "                if row < df_raw.shape[0]:\n",
    "                    date_val = df_raw.iloc[row, date_col]\n",
    "                    price_val = df_raw.iloc[row, price_col]\n",
    "                    \n",
    "                    if pd.notna(date_val) and pd.notna(price_val):\n",
    "                        try:\n",
    "                            date_obj = pd.to_datetime(date_val)\n",
    "                            stock_prices[date_obj] = float(price_val)\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            if stock_prices:\n",
    "                price_lookup[bbid] = stock_prices\n",
    "\n",
    "print(f\"Found price data for {len(price_lookup)} stocks\")\n",
    "\n",
    "# 4. Add stock price column\n",
    "complete_data['Stock_Price'] = complete_data.apply(\n",
    "    lambda row: price_lookup.get(row['BBID'], {}).get(row['Date'], np.nan), axis=1\n",
    ")\n",
    "\n",
    "# 5. Filter business days only\n",
    "print(\"Filtering business days...\")\n",
    "original_count = len(complete_data)\n",
    "complete_data = complete_data[complete_data['Date'].apply(lambda x: cal.is_working_day(x.date()))]\n",
    "print(f\"Filtered: {original_count:,} -> {len(complete_data):,} rows\")\n",
    "\n",
    "# 6. Set date as index\n",
    "complete_data.set_index('Date', inplace=True)\n",
    "complete_data.sort_index(inplace=True)\n",
    "\n",
    "# 7. Display final results\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"📊 FINAL DATAFRAME\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {complete_data.shape[0]:,} rows × {complete_data.shape[1]} columns\")\n",
    "print(f\"Date range: {complete_data.index.min().strftime('%Y-%m-%d')} to {complete_data.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nColumns:\")\n",
    "for i, col in enumerate(complete_data.columns, 1):\n",
    "    non_null = complete_data[col].notna().sum()\n",
    "    print(f\"{i:2d}. {col:<20} ({non_null:,} non-null)\")\n",
    "\n",
    "print(f\"\\nSample data:\")\n",
    "print(complete_data[['Company_Name', 'Industry', 'Stock_Price']].head())\n",
    "\n",
    "# Statistics\n",
    "price_missing = complete_data['Stock_Price'].isnull().sum()\n",
    "print(f\"\\nStock_Price missing: {price_missing:,} / {len(complete_data):,} ({price_missing/len(complete_data)*100:.1f}%)\")\n",
    "\n",
    "# Save results\n",
    "complete_data.to_excel('merged_ark_data_clean.xlsx')\n",
    "print(f\"\\n✅ Saved to merged_ark_data_clean.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b28201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 COMPLETE ARK DATA PROCESSING PIPELINE\n",
      "================================================================================\n",
      "📊 STEP 1: FILLING MISSING STOCK PRICES\n",
      "============================================================\n",
      "Found 908 records with missing stock prices\n",
      "Extracting unique ticker-date combinations...\n",
      "Valid (ticker, date) combinations to fetch: 358\n",
      "Processing ARMH (357 dates)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ARMH: possibly delisted; no price data found  (1d 2014-10-31 00:00:00 -> 2016-07-26 00:00:00) (Yahoo error = \"Data doesn't exist for startDate = 1414728000, endDate = 1469505600\")\n",
      "$NVTAQ: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NVTAQ (1 dates)...\n",
      "✅ Price filling complete: 0 prices filled\n",
      "\n",
      "📊 STEP 2: ANALYZING PRICE COVERAGE BY DATE\n",
      "============================================================\n",
      "Overall price coverage: 99.1%\n",
      "\n",
      "Worst 10 dates (price coverage):\n",
      "   2024-02-06: 34/36 (94.4%)\n",
      "   2023-03-30: 28/29 (96.6%)\n",
      "   2023-03-31: 28/29 (96.6%)\n",
      "   2023-04-03: 28/29 (96.6%)\n",
      "   2023-04-04: 28/29 (96.6%)\n",
      "   2023-04-05: 28/29 (96.6%)\n",
      "   2023-04-06: 28/29 (96.6%)\n",
      "   2023-04-10: 28/29 (96.6%)\n",
      "   2023-04-11: 28/29 (96.6%)\n",
      "   2023-04-12: 28/29 (96.6%)\n",
      "✅ No dates with <50.0% coverage found\n",
      "\n",
      "📊 STEP 3: DATA CLEANING AND WEIGHT CALCULATION\n",
      "============================================================\n",
      "Removing USD Currency entries...\n",
      "Removed 2,556 USD Currency entries\n",
      "Recalculating Market Value...\n",
      "Market Value calculated for: 112,742 records\n",
      "Calculating ETF Market Value and Weights...\n",
      "Weight calculated for: 112,686 records\n",
      "⚠️  1 dates with weight sum ≠ 1\n",
      "\n",
      "📊 STEP 4: EXPORTING FINAL DATA\n",
      "============================================================\n",
      "\n",
      "🎉 PROCESSING COMPLETE!\n",
      "================================================================================\n",
      "✅ Final data exported to 'ark_data_complete_processed.xlsx'\n",
      "📊 Final dataset: 113,650 rows × 11 columns\n",
      "📅 Date range: 2014-10-31 to 2025-05-21\n",
      "💹 Records with prices: 112,742\n",
      "📈 Overall price coverage: 99.2%\n",
      "⚖️  Average daily weight sum: 0.999620\n",
      "\n",
      "Processing steps completed:\n",
      "   1. ✅ Price filling using yfinance\n",
      "   2. ✅ Quality-based date filtering\n",
      "   3. ✅ Data cleaning and weight calculation\n",
      "   4. ✅ Excel export with formatting\n",
      "\n",
      "🚀 Ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# Complete ARK Data Processing Pipeline\n",
    "# 1. Fill missing prices → 2. Remove low quality dates → 3. Clean data & calculate weights\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "print(\"🚀 COMPLETE ARK DATA PROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FILL MISSING STOCK PRICES USING YFINANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"📊 STEP 1: FILLING MISSING STOCK PRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find records with missing stock prices\n",
    "missing_mask = complete_data['Stock_Price'].isna()\n",
    "missing_records = complete_data[missing_mask].copy()\n",
    "missing_count = len(missing_records)\n",
    "\n",
    "print(f\"Found {missing_count:,} records with missing stock prices\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    # Extract ticker and create unique combinations\n",
    "    print(\"Extracting unique ticker-date combinations...\")\n",
    "    missing_records['Ticker'] = missing_records['Name'].str.split().str[0]\n",
    "    missing_records['Date'] = missing_records.index\n",
    "    \n",
    "    # Filter valid tickers\n",
    "    def is_valid_ticker(ticker):\n",
    "        if pd.isna(ticker):\n",
    "            return False\n",
    "        ticker_str = str(ticker).upper()\n",
    "        if (ticker_str[0].isdigit() or 'XX' in ticker_str or len(ticker_str) > 6):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    valid_missing = missing_records[missing_records['Ticker'].apply(is_valid_ticker)].copy()\n",
    "    unique_combinations = valid_missing[['Ticker', 'Date']].drop_duplicates()\n",
    "    print(f\"Valid (ticker, date) combinations to fetch: {len(unique_combinations):,}\")\n",
    "    \n",
    "    # Fetch prices by ticker\n",
    "    failed_tickers = set()\n",
    "    filled_prices = {}\n",
    "    successful_count = 0\n",
    "    \n",
    "    ticker_groups = unique_combinations.groupby('Ticker')\n",
    "    \n",
    "    for ticker, group in ticker_groups:\n",
    "        if ticker in failed_tickers:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {ticker} ({len(group)} dates)...\")\n",
    "        \n",
    "        try:\n",
    "            dates = group['Date'].tolist()\n",
    "            min_date = min(dates)\n",
    "            max_date = max(dates)\n",
    "            \n",
    "            stock = yf.Ticker(ticker)\n",
    "            hist = stock.history(start=min_date, end=max_date + timedelta(days=1))\n",
    "            \n",
    "            if hist.empty:\n",
    "                failed_tickers.add(ticker)\n",
    "                continue\n",
    "            \n",
    "            ticker_success = 0\n",
    "            for date in dates:\n",
    "                date_str = date.strftime('%Y-%m-%d')\n",
    "                matching_rows = hist[hist.index.strftime('%Y-%m-%d') == date_str]\n",
    "                \n",
    "                if not matching_rows.empty:\n",
    "                    price = matching_rows['Close'].iloc[0]\n",
    "                    filled_prices[(ticker, date)] = price\n",
    "                    ticker_success += 1\n",
    "            \n",
    "            if ticker_success > 0:\n",
    "                successful_count += ticker_success\n",
    "                print(f\"   ✅ Found {ticker_success}/{len(dates)} prices for {ticker}\")\n",
    "            else:\n",
    "                failed_tickers.add(ticker)\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_tickers.add(ticker)\n",
    "    \n",
    "    # Update dataframe with filled prices\n",
    "    if successful_count > 0:\n",
    "        print(\"Updating dataframe with filled prices...\")\n",
    "        for idx, row in valid_missing.iterrows():\n",
    "            ticker = row['Ticker']\n",
    "            date = row['Date']\n",
    "            if (ticker, date) in filled_prices:\n",
    "                complete_data.loc[idx, 'Stock_Price'] = filled_prices[(ticker, date)]\n",
    "    \n",
    "    print(f\"✅ Price filling complete: {successful_count:,} prices filled\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: ANALYZE AND REMOVE DATES WITH POOR PRICE COVERAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n📊 STEP 2: ANALYZING PRICE COVERAGE BY DATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate price coverage by date\n",
    "daily_stats = complete_data.groupby(complete_data.index).agg({\n",
    "    'Stock_Price': lambda x: x.notna().sum(),\n",
    "    'Name': 'count'\n",
    "})\n",
    "daily_stats.columns = ['Tickers_With_Price', 'Total_Tickers']\n",
    "daily_stats['Price_Coverage_Pct'] = (daily_stats['Tickers_With_Price'] / daily_stats['Total_Tickers'] * 100).round(1)\n",
    "\n",
    "print(f\"Overall price coverage: {daily_stats['Price_Coverage_Pct'].mean():.1f}%\")\n",
    "\n",
    "# Show worst dates\n",
    "worst_dates = daily_stats.nsmallest(10, 'Price_Coverage_Pct')\n",
    "print(f\"\\nWorst 10 dates (price coverage):\")\n",
    "for date, row in worst_dates.iterrows():\n",
    "    print(f\"   {date.strftime('%Y-%m-%d')}: {row['Tickers_With_Price']:.0f}/{row['Total_Tickers']:.0f} ({row['Price_Coverage_Pct']:.1f}%)\")\n",
    "\n",
    "# Remove dates with <50% coverage\n",
    "threshold = 50.0\n",
    "dates_to_remove = daily_stats[daily_stats['Price_Coverage_Pct'] < threshold].index\n",
    "\n",
    "if len(dates_to_remove) > 0:\n",
    "    print(f\"\\nRemoving {len(dates_to_remove)} dates with <{threshold}% price coverage...\")\n",
    "    original_count = len(complete_data)\n",
    "    complete_data = complete_data[~complete_data.index.isin(dates_to_remove)]\n",
    "    removed_count = original_count - len(complete_data)\n",
    "    print(f\"✅ Removed {removed_count:,} records from {len(dates_to_remove):,} dates\")\n",
    "else:\n",
    "    print(f\"✅ No dates with <{threshold}% coverage found\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: DATA CLEANING AND WEIGHT CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n📊 STEP 3: DATA CLEANING AND WEIGHT CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove USD entries\n",
    "print(\"Removing USD Currency entries...\")\n",
    "original_count = len(complete_data)\n",
    "complete_data = complete_data[complete_data['Name'] != 'USD Curncy']\n",
    "usd_removed = original_count - len(complete_data)\n",
    "print(f\"Removed {usd_removed:,} USD Currency entries\")\n",
    "\n",
    "# Recalculate Market Value\n",
    "print(\"Recalculating Market Value...\")\n",
    "complete_data['Market Value'] = np.nan\n",
    "mask = (complete_data['Position'].notna()) & (complete_data['Stock_Price'].notna())\n",
    "complete_data.loc[mask, 'Market Value'] = complete_data.loc[mask, 'Position'] * complete_data.loc[mask, 'Stock_Price']\n",
    "calculated_count = complete_data['Market Value'].notna().sum()\n",
    "print(f\"Market Value calculated for: {calculated_count:,} records\")\n",
    "\n",
    "# Calculate ETF Market Value and Weights\n",
    "print(\"Calculating ETF Market Value and Weights...\")\n",
    "complete_data['Weight'] = np.nan\n",
    "\n",
    "# Daily ETF market values\n",
    "daily_total_mv = complete_data.groupby(complete_data.index)['Market Value'].sum()\n",
    "complete_data['ETF Market Value'] = complete_data.index.map(daily_total_mv)\n",
    "\n",
    "# Calculate weights\n",
    "weight_mask = ((complete_data['Market Value'].notna()) & \n",
    "              (complete_data['ETF Market Value'].notna()) & \n",
    "              (complete_data['ETF Market Value'] != 0))\n",
    "complete_data.loc[weight_mask, 'Weight'] = (\n",
    "    complete_data.loc[weight_mask, 'Market Value'] / \n",
    "    complete_data.loc[weight_mask, 'ETF Market Value']\n",
    ")\n",
    "\n",
    "weight_calculated = complete_data['Weight'].notna().sum()\n",
    "print(f\"Weight calculated for: {weight_calculated:,} records\")\n",
    "\n",
    "# Validate weights\n",
    "weight_sums = complete_data.groupby(complete_data.index)['Weight'].sum()\n",
    "tolerance = 0.001\n",
    "problematic_dates = weight_sums[abs(weight_sums - 1.0) > tolerance]\n",
    "\n",
    "if len(problematic_dates) > 0:\n",
    "    print(f\"⚠️  {len(problematic_dates)} dates with weight sum ≠ 1\")\n",
    "else:\n",
    "    print(\"✅ All dates have weight sum ≈ 1.0\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: FINAL EXPORT TO EXCEL\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n📊 STEP 4: EXPORTING FINAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare for export\n",
    "export_data = complete_data.reset_index()\n",
    "export_data['Date'] = export_data['Date'].dt.date\n",
    "\n",
    "# Remove unwanted columns\n",
    "columns_to_remove = ['CUSIP', 'ISIN']\n",
    "for col in columns_to_remove:\n",
    "    if col in export_data.columns:\n",
    "        export_data = export_data.drop(columns=[col])\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['Date', 'Name', 'Company_Name', 'Industry', 'Position', 'Stock_Price', \n",
    "                'Market Value', 'ETF Market Value', 'Weight', 'BBID', 'Fund']\n",
    "\n",
    "existing_cols = [col for col in column_order if col in export_data.columns]\n",
    "other_cols = [col for col in export_data.columns if col not in column_order]\n",
    "final_column_order = existing_cols + other_cols\n",
    "export_data = export_data[final_column_order]\n",
    "\n",
    "# Export to Excel\n",
    "filename = 'ark_data_complete_processed.xlsx'\n",
    "with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "    export_data.to_excel(writer, sheet_name='ARK_Data_Final', index=False)\n",
    "    \n",
    "    # Auto-adjust column widths\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['ARK_Data_Final']\n",
    "    \n",
    "    for column in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(str(cell.value))\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = min(max_length + 2, 50)\n",
    "        worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n🎉 PROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ Final data exported to '{filename}'\")\n",
    "print(f\"📊 Final dataset: {len(export_data):,} rows × {len(export_data.columns)} columns\")\n",
    "print(f\"📅 Date range: {export_data['Date'].min()} to {export_data['Date'].max()}\")\n",
    "print(f\"💹 Records with prices: {complete_data['Stock_Price'].notna().sum():,}\")\n",
    "print(f\"📈 Overall price coverage: {complete_data['Stock_Price'].notna().sum() / len(complete_data) * 100:.1f}%\")\n",
    "print(f\"⚖️  Average daily weight sum: {weight_sums.mean():.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b177f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 113650 entries, 2014-10-31 to 2025-05-21\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Name              113650 non-null  object \n",
      " 1   Weight            112686 non-null  float64\n",
      " 2   Position          113650 non-null  float64\n",
      " 3   Market Value      112742 non-null  float64\n",
      " 4   CUSIP             104313 non-null  object \n",
      " 5   BBID              113650 non-null  object \n",
      " 6   ISIN              113115 non-null  object \n",
      " 7   Fund Flows        113650 non-null  float64\n",
      " 8   Company_Name      113650 non-null  object \n",
      " 9   Industry          109860 non-null  object \n",
      " 10  Stock_Price       112742 non-null  float64\n",
      " 11  ETF Market Value  113650 non-null  float64\n",
      "dtypes: float64(6), object(6)\n",
      "memory usage: 11.3+ MB\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Position</th>\n",
       "      <th>Market Value</th>\n",
       "      <th>CUSIP</th>\n",
       "      <th>BBID</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Fund Flows</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Stock_Price</th>\n",
       "      <th>ETF Market Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>MELI US Equity</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>424.0</td>\n",
       "      <td>57727.60</td>\n",
       "      <td>58733R102</td>\n",
       "      <td>BBG000GQPB11</td>\n",
       "      <td>US58733R1023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MercadoLibre Inc</td>\n",
       "      <td>Broadline Retail</td>\n",
       "      <td>136.1500</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>ADSK US Equity</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>80556.00</td>\n",
       "      <td>52769106</td>\n",
       "      <td>BBG000BM7HL0</td>\n",
       "      <td>US0527691069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Autodesk Inc</td>\n",
       "      <td>Software</td>\n",
       "      <td>57.5400</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>PRLB US Equity</td>\n",
       "      <td>0.026278</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>103546.08</td>\n",
       "      <td>743713109</td>\n",
       "      <td>BBG000BT13B3</td>\n",
       "      <td>US7437131094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Proto Labs Inc</td>\n",
       "      <td>Machinery</td>\n",
       "      <td>65.3700</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>WOLF US Equity</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>38279.68</td>\n",
       "      <td>977852102</td>\n",
       "      <td>BBG000BG14P4</td>\n",
       "      <td>US9778521024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wolfspeed Inc</td>\n",
       "      <td>Semiconductors &amp; Semiconductor Equipment</td>\n",
       "      <td>31.4800</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>NVDA US Equity</td>\n",
       "      <td>0.030784</td>\n",
       "      <td>248320.0</td>\n",
       "      <td>121304.32</td>\n",
       "      <td>67066G104</td>\n",
       "      <td>BBG000BBJQV0</td>\n",
       "      <td>US67066G1040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NVIDIA Corp</td>\n",
       "      <td>Semiconductors &amp; Semiconductor Equipment</td>\n",
       "      <td>0.4885</td>\n",
       "      <td>3.940483e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name    Weight  Position  Market Value      CUSIP  \\\n",
       "Date                                                                      \n",
       "2014-10-31  MELI US Equity  0.014650     424.0      57727.60  58733R102   \n",
       "2014-10-31  ADSK US Equity  0.020443    1400.0      80556.00   52769106   \n",
       "2014-10-31  PRLB US Equity  0.026278    1584.0     103546.08  743713109   \n",
       "2014-10-31  WOLF US Equity  0.009714    1216.0      38279.68  977852102   \n",
       "2014-10-31  NVDA US Equity  0.030784  248320.0     121304.32  67066G104   \n",
       "\n",
       "                    BBID          ISIN  Fund Flows      Company_Name  \\\n",
       "Date                                                                   \n",
       "2014-10-31  BBG000GQPB11  US58733R1023         0.0  MercadoLibre Inc   \n",
       "2014-10-31  BBG000BM7HL0  US0527691069         0.0      Autodesk Inc   \n",
       "2014-10-31  BBG000BT13B3  US7437131094         0.0    Proto Labs Inc   \n",
       "2014-10-31  BBG000BG14P4  US9778521024         0.0     Wolfspeed Inc   \n",
       "2014-10-31  BBG000BBJQV0  US67066G1040         0.0       NVIDIA Corp   \n",
       "\n",
       "                                            Industry  Stock_Price  \\\n",
       "Date                                                                \n",
       "2014-10-31                          Broadline Retail     136.1500   \n",
       "2014-10-31                                  Software      57.5400   \n",
       "2014-10-31                                 Machinery      65.3700   \n",
       "2014-10-31  Semiconductors & Semiconductor Equipment      31.4800   \n",
       "2014-10-31  Semiconductors & Semiconductor Equipment       0.4885   \n",
       "\n",
       "            ETF Market Value  \n",
       "Date                          \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  \n",
       "2014-10-31      3.940483e+06  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"DataFrame Info:\")\n",
    "complete_data.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2eafb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ARK SMALL POSITIONS ANALYSIS - PHASE 1\n",
      "================================================================================\n",
      "📊 STEP 1: IDENTIFYING SMALL POSITIONS (<1%)\n",
      "============================================================\n",
      "Total records: 113,650\n",
      "Small positions (<1%): 35,890 (31.6%)\n",
      "Large positions (≥1%): 76,796 (67.6%)\n",
      "\n",
      "📈 STEP 2: CALCULATING DAILY RETURNS\n",
      "============================================================\n",
      "Small positions with valid returns: 35,524\n",
      "Large positions with valid returns: 76,385\n",
      "\n",
      "📊 STEP 3: STATISTICAL ANALYSIS - MEAN & MEDIAN\n",
      "============================================================\n",
      "SMALL POSITIONS (<1%) STATISTICS:\n",
      "  Mean daily return: -0.0062 (-0.62%)\n",
      "  Median daily return: 0.0000 (0.00%)\n",
      "  Standard deviation: 0.1526 (15.26%)\n",
      "\n",
      "LARGE POSITIONS (≥1%) STATISTICS:\n",
      "  Mean daily return: 0.0020 (0.20%)\n",
      "  Median daily return: 0.0004 (0.04%)\n",
      "  Standard deviation: 0.0931 (9.31%)\n",
      "\n",
      "📊 STEP 4: DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "SMALL POSITIONS DISTRIBUTION:\n",
      "   1th percentile: -0.7133 (-71.33%)\n",
      "   5th percentile: -0.0799 (-7.99%)\n",
      "  10th percentile: -0.0472 (-4.72%)\n",
      "  25th percentile: -0.0178 (-1.78%)\n",
      "  50th percentile: 0.0000 (0.00%)\n",
      "  75th percentile: 0.0145 (1.45%)\n",
      "  90th percentile: 0.0389 (3.89%)\n",
      "  95th percentile: 0.0637 (6.37%)\n",
      "  99th percentile: 0.4369 (43.69%)\n",
      "\n",
      "LARGE POSITIONS DISTRIBUTION:\n",
      "   1th percentile: -0.1294 (-12.94%)\n",
      "   5th percentile: -0.0588 (-5.88%)\n",
      "  10th percentile: -0.0398 (-3.98%)\n",
      "  25th percentile: -0.0169 (-1.69%)\n",
      "  50th percentile: 0.0004 (0.04%)\n",
      "  75th percentile: 0.0179 (1.79%)\n",
      "  90th percentile: 0.0408 (4.08%)\n",
      "  95th percentile: 0.0612 (6.12%)\n",
      "  99th percentile: 0.1406 (14.06%)\n",
      "\n",
      "🏆 STEP 5: TOP 1% AND BOTTOM 1% RETURNS\n",
      "============================================================\n",
      "SMALL POSITIONS:\n",
      "  Top 1% threshold: 0.4369 (43.69%)\n",
      "  Bottom 1% threshold: -0.7133 (-71.33%)\n",
      "\n",
      "LARGE POSITIONS:\n",
      "  Top 1% threshold: 0.1406 (14.06%)\n",
      "  Bottom 1% threshold: -0.1294 (-12.94%)\n",
      "\n",
      "💰 STEP 6: MARKET VALUE ANALYSIS\n",
      "============================================================\n",
      "📊 CALCULATING DAILY MARKET VALUE CHANGES\n",
      "----------------------------------------\n",
      "Small positions daily changes: 35,435 observations\n",
      "Large positions daily changes: 76,307 observations\n",
      "\n",
      "💸 DAILY MARKET VALUE IMPACT:\n",
      "----------------------------------------\n",
      "SMALL POSITIONS (<1%) DAILY MARKET VALUE IMPACT:\n",
      "  Average daily change: $-211,874.13\n",
      "  Median daily change: $0.00\n",
      "  Total days analyzed: 35,435\n",
      "\n",
      "LARGE POSITIONS (≥1%) DAILY MARKET VALUE IMPACT:\n",
      "  Average daily change: $500,346.76\n",
      "  Median daily change: $130.95\n",
      "  Total days analyzed: 76,307\n",
      "\n",
      "📈 CUMULATIVE GAINS/LOSSES:\n",
      "----------------------------------------\n",
      "TOTAL MARKET VALUE IMPACT OVER ENTIRE PERIOD:\n",
      "  Small positions total change: $-7,507,759,683.93\n",
      "  Large positions total change: $38,179,960,068.31\n",
      "  ❌ Small positions LOST money: $7,507,759,683.93\n",
      "  ✅ Large positions MADE money: $38,179,960,068.31\n",
      "\n",
      "🏆 WIN/LOSS DAYS ANALYSIS:\n",
      "----------------------------------------\n",
      "SMALL POSITIONS WIN/LOSS RECORD:\n",
      "  Positive days: 15,815 (44.6%)\n",
      "  Negative days: 16,908 (55.4%)\n",
      "  Average gain on positive days: $1,185,072.18\n",
      "  Average loss on negative days: $-1,552,500.37\n",
      "\n",
      "LARGE POSITIONS WIN/LOSS RECORD:\n",
      "  Positive days: 38,554 (50.5%)\n",
      "  Negative days: 37,141 (49.5%)\n",
      "  Average gain on positive days: $7,631,447.59\n",
      "  Average loss on negative days: $-6,893,806.59\n",
      "\n",
      "🎯 EXTREME MARKET VALUE DAYS:\n",
      "----------------------------------------\n",
      "EXTREME DAYS:\n",
      "  Small positions best day: $91,285,790.82\n",
      "  Small positions worst day: $-92,820,333.51\n",
      "  Large positions best day: $722,494,093.25\n",
      "  Large positions worst day: $-570,097,140.04\n",
      "\n",
      "TOP 1% AND BOTTOM 1% THRESHOLDS:\n",
      "  Small positions - Top 1% days: $9,270,287.02\n",
      "  Small positions - Bottom 1% days: $-11,892,441.78\n",
      "  Large positions - Top 1% days: $54,661,288.14\n",
      "  Large positions - Bottom 1% days: $-51,285,174.47\n",
      "\n",
      "📋 MARKET VALUE SUMMARY TABLE:\n",
      "----------------------------------------\n",
      "                  Metric Small Positions (<1%) Large Positions (≥1%)\n",
      "Average Daily Change ($)           -211,874.13            500,346.76\n",
      " Median Daily Change ($)                  0.00                130.95\n",
      "        Total Change ($)     -7,507,759,683.93     38,179,960,068.31\n",
      "            Win Rate (%)                  44.6                  50.5\n",
      "            Best Day ($)         91,285,790.82        722,494,093.25\n",
      "           Worst Day ($)        -92,820,333.51       -570,097,140.04\n",
      "\n",
      "🎯 STEP 7: PHASE 1 SUMMARY\n",
      "================================================================================\n",
      "📈 HISTORICAL PERFORMANCE SUMMARY:\n",
      "  Small positions mean return: -0.617% daily\n",
      "  Large positions mean return: 0.205% daily\n",
      "  Performance difference: -0.822% daily\n",
      "\n",
      "📊 DISTRIBUTION CHARACTERISTICS:\n",
      "  Small positions volatility: 15.26%\n",
      "  Large positions volatility: 9.31%\n",
      "\n",
      "🏆 EXTREME RETURNS:\n",
      "  Small positions - Top 1%: 43.69%\n",
      "  Small positions - Bottom 1%: -71.33%\n",
      "  Large positions - Top 1%: 14.06%\n",
      "  Large positions - Bottom 1%: -12.94%\n",
      "\n",
      "💰 MARKET VALUE CONCLUSION:\n",
      "  ❌ SMALL POSITIONS UNDERPERFORMED in absolute dollar terms!\n",
      "     Small positions lost $45,687,719,752.24 compared to large positions\n",
      "\n",
      "💡 KEY INSIGHTS:\n",
      "   • Small positions average $-211,874.13 per day\n",
      "   • Large positions average $500,346.76 per day\n",
      "   • Small positions win rate: 44.6%\n",
      "   • Large positions win rate: 50.5%\n",
      "\n",
      "✅ Phase 1 Complete!\n",
      "📊 Ready for Phase 2: Holding Period and Stability Analysis\n"
     ]
    }
   ],
   "source": [
    "# ARK Portfolio Analysis - Small Positions (<1%) Performance Study\n",
    "# Phase 1: Statistical Analysis of Less Than 1% Positions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🔍 ARK SMALL POSITIONS ANALYSIS - PHASE 1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Filter positions with weight < 1%\n",
    "print(\"📊 STEP 1: IDENTIFYING SMALL POSITIONS (<1%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create small positions dataset\n",
    "small_positions = complete_data[complete_data['Weight'] < 0.01].copy()\n",
    "large_positions = complete_data[complete_data['Weight'] >= 0.01].copy()\n",
    "\n",
    "print(f\"Total records: {len(complete_data):,}\")\n",
    "print(f\"Small positions (<1%): {len(small_positions):,} ({len(small_positions)/len(complete_data)*100:.1f}%)\")\n",
    "print(f\"Large positions (≥1%): {len(large_positions):,} ({len(large_positions)/len(complete_data)*100:.1f}%)\")\n",
    "\n",
    "# Step 2: Calculate daily returns for analysis\n",
    "print(f\"\\n📈 STEP 2: CALCULATING DAILY RETURNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_returns(df):\n",
    "    \"\"\"Calculate daily returns for each stock using vectorized operations\"\"\"\n",
    "    df = df.copy().sort_index()\n",
    "    df['Price_Return'] = df.groupby('Name')['Stock_Price'].pct_change()\n",
    "    return df\n",
    "\n",
    "# Calculate returns for all data at once, then split\n",
    "complete_data_with_returns = calculate_returns(complete_data)\n",
    "\n",
    "# Filter out invalid returns\n",
    "valid_returns_mask = (\n",
    "    (complete_data_with_returns['Price_Return'].notna()) & \n",
    "    (abs(complete_data_with_returns['Price_Return']) < 2.0)  # Remove >200% daily moves\n",
    ")\n",
    "\n",
    "complete_data_clean = complete_data_with_returns[valid_returns_mask]\n",
    "\n",
    "# Split into small and large positions\n",
    "small_positions = complete_data_clean[complete_data_clean['Weight'] < 0.01].copy()\n",
    "large_positions = complete_data_clean[complete_data_clean['Weight'] >= 0.01].copy()\n",
    "\n",
    "print(f\"Small positions with valid returns: {len(small_positions):,}\")\n",
    "print(f\"Large positions with valid returns: {len(large_positions):,}\")\n",
    "\n",
    "# Step 3: Statistical Analysis - Mean, Median\n",
    "print(f\"\\n📊 STEP 3: STATISTICAL ANALYSIS - MEAN & MEDIAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "small_returns = small_positions['Price_Return']\n",
    "large_returns = large_positions['Price_Return']\n",
    "\n",
    "print(\"SMALL POSITIONS (<1%) STATISTICS:\")\n",
    "print(f\"  Mean daily return: {small_returns.mean():.4f} ({small_returns.mean()*100:.2f}%)\")\n",
    "print(f\"  Median daily return: {small_returns.median():.4f} ({small_returns.median()*100:.2f}%)\")\n",
    "print(f\"  Standard deviation: {small_returns.std():.4f} ({small_returns.std()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nLARGE POSITIONS (≥1%) STATISTICS:\")\n",
    "print(f\"  Mean daily return: {large_returns.mean():.4f} ({large_returns.mean()*100:.2f}%)\")\n",
    "print(f\"  Median daily return: {large_returns.median():.4f} ({large_returns.median()*100:.2f}%)\")\n",
    "print(f\"  Standard deviation: {large_returns.std():.4f} ({large_returns.std()*100:.2f}%)\")\n",
    "\n",
    "# Step 4: Distribution Analysis\n",
    "print(f\"\\n📊 STEP 4: DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "\n",
    "print(\"SMALL POSITIONS DISTRIBUTION:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(small_returns, p)\n",
    "    print(f\"  {p:2d}th percentile: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nLARGE POSITIONS DISTRIBUTION:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(large_returns, p)\n",
    "    print(f\"  {p:2d}th percentile: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "# Step 5: Top 1% and Bottom 1% Returns\n",
    "print(f\"\\n🏆 STEP 5: TOP 1% AND BOTTOM 1% RETURNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "small_top_1pct = small_returns.quantile(0.99)\n",
    "small_bottom_1pct = small_returns.quantile(0.01)\n",
    "large_top_1pct = large_returns.quantile(0.99)\n",
    "large_bottom_1pct = large_returns.quantile(0.01)\n",
    "\n",
    "print(f\"SMALL POSITIONS:\")\n",
    "print(f\"  Top 1% threshold: {small_top_1pct:.4f} ({small_top_1pct*100:.2f}%)\")\n",
    "print(f\"  Bottom 1% threshold: {small_bottom_1pct:.4f} ({small_bottom_1pct*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nLARGE POSITIONS:\")\n",
    "print(f\"  Top 1% threshold: {large_top_1pct:.4f} ({large_top_1pct*100:.2f}%)\")\n",
    "print(f\"  Bottom 1% threshold: {large_bottom_1pct:.4f} ({large_bottom_1pct*100:.2f}%)\")\n",
    "\n",
    "# Step 6: Market Value Analysis\n",
    "print(f\"\\n💰 STEP 6: MARKET VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate daily market value changes\n",
    "print(\"📊 CALCULATING DAILY MARKET VALUE CHANGES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate daily market value change = Previous Market Value * Daily Return\n",
    "small_positions['Daily_MV_Change'] = small_positions['Market Value'] * small_positions['Price_Return']\n",
    "large_positions['Daily_MV_Change'] = large_positions['Market Value'] * large_positions['Price_Return']\n",
    "\n",
    "# Remove extreme outliers and NaN values\n",
    "small_mv_clean = small_positions['Daily_MV_Change'].dropna()\n",
    "large_mv_clean = large_positions['Daily_MV_Change'].dropna()\n",
    "\n",
    "# Filter out extreme values (likely data errors)\n",
    "small_mv_clean = small_mv_clean[abs(small_mv_clean) < small_mv_clean.quantile(0.999)]\n",
    "large_mv_clean = large_mv_clean[abs(large_mv_clean) < large_mv_clean.quantile(0.999)]\n",
    "\n",
    "print(f\"Small positions daily changes: {len(small_mv_clean):,} observations\")\n",
    "print(f\"Large positions daily changes: {len(large_mv_clean):,} observations\")\n",
    "\n",
    "# Daily market value statistics\n",
    "print(f\"\\n💸 DAILY MARKET VALUE IMPACT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "small_daily_avg = small_mv_clean.mean()\n",
    "large_daily_avg = large_mv_clean.mean()\n",
    "small_daily_median = small_mv_clean.median()\n",
    "large_daily_median = large_mv_clean.median()\n",
    "\n",
    "print(f\"SMALL POSITIONS (<1%) DAILY MARKET VALUE IMPACT:\")\n",
    "print(f\"  Average daily change: ${small_daily_avg:,.2f}\")\n",
    "print(f\"  Median daily change: ${small_daily_median:,.2f}\")\n",
    "print(f\"  Total days analyzed: {len(small_mv_clean):,}\")\n",
    "\n",
    "print(f\"\\nLARGE POSITIONS (≥1%) DAILY MARKET VALUE IMPACT:\")\n",
    "print(f\"  Average daily change: ${large_daily_avg:,.2f}\")\n",
    "print(f\"  Median daily change: ${large_daily_median:,.2f}\")\n",
    "print(f\"  Total days analyzed: {len(large_mv_clean):,}\")\n",
    "\n",
    "# Cumulative gains/losses\n",
    "print(f\"\\n📈 CUMULATIVE GAINS/LOSSES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "small_total_change = small_mv_clean.sum()\n",
    "large_total_change = large_mv_clean.sum()\n",
    "\n",
    "print(f\"TOTAL MARKET VALUE IMPACT OVER ENTIRE PERIOD:\")\n",
    "print(f\"  Small positions total change: ${small_total_change:,.2f}\")\n",
    "print(f\"  Large positions total change: ${large_total_change:,.2f}\")\n",
    "\n",
    "if small_total_change > 0:\n",
    "    print(f\"  ✅ Small positions MADE money: ${small_total_change:,.2f}\")\n",
    "else:\n",
    "    print(f\"  ❌ Small positions LOST money: ${abs(small_total_change):,.2f}\")\n",
    "\n",
    "if large_total_change > 0:\n",
    "    print(f\"  ✅ Large positions MADE money: ${large_total_change:,.2f}\")\n",
    "else:\n",
    "    print(f\"  ❌ Large positions LOST money: ${abs(large_total_change):,.2f}\")\n",
    "\n",
    "# Win/Loss days analysis\n",
    "print(f\"\\n🏆 WIN/LOSS DAYS ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "small_positive_days = (small_mv_clean > 0).sum()\n",
    "small_negative_days = (small_mv_clean < 0).sum()\n",
    "small_win_rate = small_positive_days / len(small_mv_clean)\n",
    "\n",
    "large_positive_days = (large_mv_clean > 0).sum()\n",
    "large_negative_days = (large_mv_clean < 0).sum()\n",
    "large_win_rate = large_positive_days / len(large_mv_clean)\n",
    "\n",
    "print(f\"SMALL POSITIONS WIN/LOSS RECORD:\")\n",
    "print(f\"  Positive days: {small_positive_days:,} ({small_win_rate*100:.1f}%)\")\n",
    "print(f\"  Negative days: {small_negative_days:,} ({(1-small_win_rate)*100:.1f}%)\")\n",
    "print(f\"  Average gain on positive days: ${small_mv_clean[small_mv_clean > 0].mean():,.2f}\")\n",
    "print(f\"  Average loss on negative days: ${small_mv_clean[small_mv_clean < 0].mean():,.2f}\")\n",
    "\n",
    "print(f\"\\nLARGE POSITIONS WIN/LOSS RECORD:\")\n",
    "print(f\"  Positive days: {large_positive_days:,} ({large_win_rate*100:.1f}%)\")\n",
    "print(f\"  Negative days: {large_negative_days:,} ({(1-large_win_rate)*100:.1f}%)\")\n",
    "print(f\"  Average gain on positive days: ${large_mv_clean[large_mv_clean > 0].mean():,.2f}\")\n",
    "print(f\"  Average loss on negative days: ${large_mv_clean[large_mv_clean < 0].mean():,.2f}\")\n",
    "\n",
    "# Extreme days analysis\n",
    "print(f\"\\n🎯 EXTREME MARKET VALUE DAYS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Best and worst days\n",
    "small_best_day = small_mv_clean.max()\n",
    "small_worst_day = small_mv_clean.min()\n",
    "large_best_day = large_mv_clean.max()\n",
    "large_worst_day = large_mv_clean.min()\n",
    "\n",
    "print(f\"EXTREME DAYS:\")\n",
    "print(f\"  Small positions best day: ${small_best_day:,.2f}\")\n",
    "print(f\"  Small positions worst day: ${small_worst_day:,.2f}\")\n",
    "print(f\"  Large positions best day: ${large_best_day:,.2f}\")\n",
    "print(f\"  Large positions worst day: ${large_worst_day:,.2f}\")\n",
    "\n",
    "# Top 1% and bottom 1% days\n",
    "small_top_1pct_mv = small_mv_clean.quantile(0.99)\n",
    "small_bottom_1pct_mv = small_mv_clean.quantile(0.01)\n",
    "large_top_1pct_mv = large_mv_clean.quantile(0.99)\n",
    "large_bottom_1pct_mv = large_mv_clean.quantile(0.01)\n",
    "\n",
    "print(f\"\\nTOP 1% AND BOTTOM 1% THRESHOLDS:\")\n",
    "print(f\"  Small positions - Top 1% days: ${small_top_1pct_mv:,.2f}\")\n",
    "print(f\"  Small positions - Bottom 1% days: ${small_bottom_1pct_mv:,.2f}\")\n",
    "print(f\"  Large positions - Top 1% days: ${large_top_1pct_mv:,.2f}\")\n",
    "print(f\"  Large positions - Bottom 1% days: ${large_bottom_1pct_mv:,.2f}\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n📋 MARKET VALUE SUMMARY TABLE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "summary_mv_data = {\n",
    "    'Metric': ['Average Daily Change ($)', 'Median Daily Change ($)', 'Total Change ($)',\n",
    "               'Win Rate (%)', 'Best Day ($)', 'Worst Day ($)'],\n",
    "    'Small Positions (<1%)': [\n",
    "        f\"{small_daily_avg:,.2f}\",\n",
    "        f\"{small_daily_median:,.2f}\",\n",
    "        f\"{small_total_change:,.2f}\",\n",
    "        f\"{small_win_rate*100:.1f}\",\n",
    "        f\"{small_best_day:,.2f}\",\n",
    "        f\"{small_worst_day:,.2f}\"\n",
    "    ],\n",
    "    'Large Positions (≥1%)': [\n",
    "        f\"{large_daily_avg:,.2f}\",\n",
    "        f\"{large_daily_median:,.2f}\",\n",
    "        f\"{large_total_change:,.2f}\",\n",
    "        f\"{large_win_rate*100:.1f}\",\n",
    "        f\"{large_best_day:,.2f}\",\n",
    "        f\"{large_worst_day:,.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_mv_df = pd.DataFrame(summary_mv_data)\n",
    "print(summary_mv_df.to_string(index=False))\n",
    "\n",
    "# Step 7: Phase 1 Summary\n",
    "print(f\"\\n🎯 STEP 7: PHASE 1 SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "performance_diff = small_returns.mean() - large_returns.mean()\n",
    "\n",
    "print(f\"📈 HISTORICAL PERFORMANCE SUMMARY:\")\n",
    "print(f\"  Small positions mean return: {small_returns.mean()*100:.3f}% daily\")\n",
    "print(f\"  Large positions mean return: {large_returns.mean()*100:.3f}% daily\")\n",
    "print(f\"  Performance difference: {performance_diff*100:.3f}% daily\")\n",
    "\n",
    "print(f\"\\n📊 DISTRIBUTION CHARACTERISTICS:\")\n",
    "print(f\"  Small positions volatility: {small_returns.std()*100:.2f}%\")\n",
    "print(f\"  Large positions volatility: {large_returns.std()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n🏆 EXTREME RETURNS:\")\n",
    "print(f\"  Small positions - Top 1%: {small_top_1pct*100:.2f}%\")\n",
    "print(f\"  Small positions - Bottom 1%: {small_bottom_1pct*100:.2f}%\")\n",
    "print(f\"  Large positions - Top 1%: {large_top_1pct*100:.2f}%\")\n",
    "print(f\"  Large positions - Bottom 1%: {large_bottom_1pct*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n💰 MARKET VALUE CONCLUSION:\")\n",
    "if small_total_change > large_total_change:\n",
    "    print(f\"  ✅ SMALL POSITIONS OUTPERFORMED in absolute dollar terms!\")\n",
    "    print(f\"     Small positions gained ${small_total_change - large_total_change:,.2f} more than large positions\")\n",
    "else:\n",
    "    print(f\"  ❌ SMALL POSITIONS UNDERPERFORMED in absolute dollar terms!\")\n",
    "    print(f\"     Small positions lost ${abs(small_total_change - large_total_change):,.2f} compared to large positions\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(f\"   • Small positions average ${small_daily_avg:,.2f} per day\")\n",
    "print(f\"   • Large positions average ${large_daily_avg:,.2f} per day\") \n",
    "print(f\"   • Small positions win rate: {small_win_rate*100:.1f}%\")\n",
    "print(f\"   • Large positions win rate: {large_win_rate*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n✅ Phase 1 Complete!\")\n",
    "print(f\"📊 Ready for Phase 2: Holding Period and Stability Analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
